{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "83e2dacf",
    "execution_start": 1649017009209,
    "execution_millis": 3159,
    "cell_id": "fa89cba6-3c30-4696-95fc-af58a28124cd",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 459.890625
   },
   "source": "## Import Libraries ##\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport os\nimport math as m\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics, model_selection\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nimport torchvision.models as models\n\nimport imageio\nfrom tqdm.notebook import tqdm, trange\nimport PIL",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": "/shared-libs/python3.7/py/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00001-5233ab93-4d68-43cc-84a3-5e62ad406d8a",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "c1d08775",
    "execution_start": 1649017015665,
    "execution_millis": 2,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 99
   },
   "source": "## Sending to CUDA device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Data Preprocessing",
   "metadata": {
    "cell_id": "9117b14f1a46462297a639bb7b7a86af",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 70
   }
  },
  {
   "cell_type": "markdown",
   "source": "**Importing Cleaned Data**",
   "metadata": {
    "cell_id": "ce9ceebef6154b3d99e96b741f7c692a",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 52.390625
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "4375ad046b354b528c27666d3fdd0847",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 66
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Data Augmentation**",
   "metadata": {
    "cell_id": "f7378e527af843a5afa817a018170101",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 52.390625
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "eaab79b0",
    "execution_start": 1649017018201,
    "execution_millis": 4,
    "cell_id": "00002-c0d2faf2-755b-497c-b4d0-4fe1d3004312",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 369
   },
   "source": "# (from deep_learning_2.ipynb, Dr. David E. Carlson)\n\n# training data\ndata_transforms = transforms.Compose([ #data augmentation step \n        transforms.RandomResizedCrop(224), #random croppings of the image\n        transforms.RandomHorizontalFlip(), #mirror image (usually not a random vertical flip)\n        transforms.RandomRotation(10), #rotates image\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# test data\ndata_transforms_test = transforms.Compose([\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) #our new images must match statistics of original data\n])",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "3484918b",
    "execution_start": 1649017020419,
    "execution_millis": 283,
    "cell_id": "00003-9b1aa38a-1eb7-4f48-9cd3-9d971166421c",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 288.1875
   },
   "source": "# Data loader (from deep_learning_2.ipynb, Dr. David E. Carlson)\n\ntrain_data = datasets.ImageFolder('/work/meb135/cleandata/train', transform=data_transforms)\nval_data = datasets.ImageFolder('/work/meb135/cleandata/val', transform=data_transforms_test)\ntest_data = datasets.ImageFolder('/work/meb135/cleandata/test', transform=data_transforms_test)\n\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=1, shuffle=True, num_workers=0)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=1, shuffle=False, num_workers=0)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=False, num_workers=0)",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/work/meb135/cleandata/train'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b4acc1ba990e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Data loader (from deep_learning_2.ipynb, Dr. David E. Carlson)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/work/meb135/cleandata/train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/work/meb135/cleandata/val'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_transforms_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/work/meb135/cleandata/test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_transforms_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0mis_valid_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_valid_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         )\n\u001b[1;32m    318\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    143\u001b[0m     ) -> None:\n\u001b[1;32m    144\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \"\"\"\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/work/meb135/cleandata/train'"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Model selection",
   "metadata": {
    "cell_id": "d52da43173cb43ee8183f54262096d53",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 70
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00004-62ccff78-c9d8-43d8-acf3-977b8853dfb7",
    "owner_user_id": "17cfb76b-f16c-4361-97ad-0cd872b399d7",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 174
   },
   "source": "# Implementing AlexNet (WITH TRANSFER LEARNING)\n\nmodelTypes = ['AlexNet', 'VGG', 'ResNet', 'SqueezeNet', 'DenseNet', 'Inception v3', 'GoogleNet', 'ShuffleNet v2']\nmodelArr = ['model_ft = models.alexnet(pretrained=True)', 'model_ft = models.vgg16(pretrained=True)', /\n'model_ft = models.resnet18(pretrained=True)', 'model_ft = models.squeezenet1_0(pretrained=True)', 'model_ft = models.densenet161(pretrained=True)' /\n'model_ft = models.inception_v3(pretrained=True)', 'model_ft = models.googlenet(pretrained=True)', 'model_ft = models.shufflenet_v2_x1_0(pretrained=True)']\nmodelValAcc = []",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00005-7e95eead-30c9-4c93-924e-9f1308b6c813",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 174
   },
   "source": "# Sending model to CUDA device\nmodel_ft.to(device)\n\n# Correcting number of classes to 2 (from deep_learning_2.ipynb)\nnum_ftrs = model_ft.classifier[-1].in_features # Need to know # of features coming out of the penultimate layer\nmodel_ft.classifier[-1] = nn.Linear(num_ftrs, 1) # Redefining last feature to predict binary\nmodel_ft.to(device)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00006-f63b5265-ed3c-47e6-ae91-f0909131c174",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 858
   },
   "source": "# Run CNN (from deep_learning_2.ipynb)\n\ncriterion = nn.BCEWithLogitsLoss() # binary cross entropy loss\noptimizer = torch.optim.SGD(model_ft.parameters(), lr=1e-5, momentum=0.9) # momentum smooths the dataset (impacts stochastic gradient descent)\n\ndef test_model():\n    ## Testing\n    correct = 0\n    total = len(test_data)\n    model_ft.eval() # set the model into evaluation mode, which changes the \n    # behavior of the batch norm layer so that it is not sensitive to batch size\n    with torch.no_grad():\n        # Iterate through test set minibatchs \n        for images, labels in tqdm(test_loader):\n            # Forward pass\n            inputs = images.to(device)\n            labels = labels.unsqueeze(1).to(device)\n            y = model_ft(inputs)\n\n            predictions = torch.round(torch.sigmoid(y)).long()#torch.argmax(y, dim=1)\n            correct += torch.sum((predictions == labels).float())\n    print('Test accuracy: {}'.format(correct/total))\n\n\ndef train_model(model, criterion, optimizer, num_epochs=25):\n    for epoch in trange(num_epochs):\n        for images, labels in tqdm(train_loader):\n\n            inputs = images.to(device)\n            labels = labels.float().unsqueeze(1).to(device)\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # Do the forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            # Calculate gradients and step (update parameters)\n            loss.backward()\n            optimizer.step()\n        test_model()\n        model_ft.train() # set model back to training mode (batch norm layers back to normal) after testing\n\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "e666ac2ec4f44f9f8cf0085daf1a1b6a",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "5db320ea",
    "execution_start": 1649024267765,
    "execution_millis": 12,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1152.1875
   },
   "source": "modelTypes = ['AlexNet', 'VGG', 'ResNet', 'SqueezeNet', 'DenseNet', 'Inception v3', 'GoogleNet', 'ShuffleNet v2']\nmodelArr = ['model_ft = models.alexnet(pretrained=True)', 'model_ft = models.vgg16(pretrained=True)', /\n'model_ft = models.resnet18(pretrained=True)', 'model_ft = models.squeezenet1_0(pretrained=True)', 'model_ft = models.densenet161(pretrained=True)' /\n'model_ft = models.inception_v3(pretrained=True)', 'model_ft = models.googlenet(pretrained=True)', 'model_ft = models.shufflenet_v2_x1_0(pretrained=True)']\nmodelValAcc = []\n\ndef test_model():\n    ## Testing\n    correct = 0\n    total = len(test_data)\n    model_ft.eval() # set the model into evaluation mode, which changes the \n    # behavior of the batch norm layer so that it is not sensitive to batch size\n    with torch.no_grad():\n        # Iterate through test set minibatchs \n        for images, labels in tqdm(test_loader):\n            # Forward pass\n            inputs = images.to(device)\n            labels = labels.unsqueeze(1).to(device)\n            y = model_ft(inputs)\n\n            predictions = torch.round(torch.sigmoid(y)).long()#torch.argmax(y, dim=1)\n            correct += torch.sum((predictions == labels).float())\n    print('Test accuracy: {}'.format(correct/total))\n\n\ndef train_model(model, criterion, optimizer, num_epochs=25):\n    for epoch in trange(num_epochs):\n        for images, labels in tqdm(train_loader):\n\n            inputs = images.to(device)\n            labels = labels.float().unsqueeze(1).to(device)\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # Do the forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            # Calculate gradients and step (update parameters)\n            loss.backward()\n            optimizer.step()\n        test_model()\n        model_ft.train() # set model back to training mode (batch norm layers back to normal) after testing\n\n#iterate here!!\nfor m in modelArr:\n    model_ft = eval(m)\n    \n    num_ftrs = model_ft.classifier[-1].in_features # Need to know # of features coming out of the penultimate layer\n    model_ft.classifier[-1] = nn.Linear(num_ftrs, 1) # Redefining last feature to predict binary\n    model_ft.to(device)\n\n    criterion = nn.BCEWithLogitsLoss() # binary cross entropy loss\n    optimizer = torch.optim.SGD(model_ft.parameters(), lr=1e-5, momentum=0.9) # momentum smooths the dataset (impacts stochastic gradient descent)\n    \n    train_model(model_ft, criterion, optimizer, num_epochs=3)",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-46902d1f0964>, line 2)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-46902d1f0964>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    modelArr = ['model_ft = models.alexnet(pretrained=True)', 'model_ft = models.vgg16(pretrained=True)', /\u001b[0m\n\u001b[0m                                                                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {
    "cell_id": "86c57cd6c2e04a9f9f7d7b89e6cb82ef",
    "tags": [],
    "owner_user_id": "1baf9ce8-80e5-4ca1-9411-dad5fe06c85f",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 46
   }
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=eeea15ee-1713-4850-a1d6-fce82f9c7017' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "462e1faf-da2e-425b-be81-82aa38a6a0e8",
  "deepnote_execution_queue": []
 }
}